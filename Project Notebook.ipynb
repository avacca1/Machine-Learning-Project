{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6cc7a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, r2_score\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "# nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e787473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(data, col):\n",
    "    new_data = data\n",
    "    max_num = np.max(new_data[:,col])\n",
    "    min_num = np.min(new_data[:,col])\n",
    "\n",
    "    for i in range(new_data.shape[0]):\n",
    "        new_data[i][col] = (new_data[i][col] - min_num) / (max_num - min_num)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3c84226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeData(n_samples=93240,n_features=11,dataPath=\"News_Final.csv\"):\n",
    "    data = pd.read_csv(dataPath)\n",
    "    data[\"PublishDate\"] = pd.to_datetime(data[\"PublishDate\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    data['PublishDate'] = data['PublishDate'].apply(lambda x: time.mktime(x.timetuple()))\n",
    "    data = data.to_numpy()\n",
    "    \n",
    "    labels = data[:n_samples,-3:]\n",
    "    X = data[:n_samples,:-3]\n",
    "    \n",
    "    # omitting rows with no usable labels\n",
    "    bool_matrix = np.any(labels != 0, axis=1)\n",
    "    labels = labels[bool_matrix]\n",
    "    X = X[bool_matrix]\n",
    "    \n",
    "    bool_matrix = labels[:, 1] != -1\n",
    "    labels = labels[bool_matrix]\n",
    "    X = X[bool_matrix]\n",
    "\n",
    "\n",
    "    ### changing the lables \n",
    "    minmax(labels,0)\n",
    "    minmax(labels,1)\n",
    "    minmax(labels,2)\n",
    "    minmax(X,5)\n",
    "\n",
    "    Y_new = np.zeros(labels.shape[0])\n",
    "    for i in range(labels.shape[0]):\n",
    "        if labels[i][0] == -1 and labels[i][1] == -1 and labels[i][2] == -1:\n",
    "            Y_new[i] = -1\n",
    "\n",
    "        if labels[i][0] >= labels[i][1] and labels[i][0] >= labels[i][1]:\n",
    "            Y_new[i] = 0\n",
    "        elif labels[i][1] >= labels[i][0] and labels[i][1] >= labels[i][2]:\n",
    "            Y_new[i] = 1\n",
    "        elif labels[i][2] >= labels[i][1] and labels[i][2] >= labels[i][0]:\n",
    "            Y_new[i] = 2\n",
    "\n",
    "    labels = Y_new\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    return X,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bfa896f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X, labels, test_size, seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    indices = range(X.shape[0])\n",
    "    train_indices = np.random.choice(indices,size = int(len(indices) * (1-test_size)),replace=False)\n",
    "    test_indices = np.setdiff1d(indices,train_indices)\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]  \n",
    "    \n",
    "    y_train = labels[train_indices]\n",
    "    y_test = labels[test_indices]\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "acb0c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.internals.blocks import final\n",
    "class NewsData(object):\n",
    "    def __init__(self):\n",
    "        self.X, self.labels = makeData(1000)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = splitData(self.X,self.labels, 0.2, 2023)\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf = None\n",
    "        self.corpus = []\n",
    "        \n",
    "    def OneHotEncodeTrain(self, index):\n",
    "        col = np.array(self.X_train[:,index]).reshape(-1, 1)\n",
    "        self.encoder.fit(col)\n",
    "        one_hot_data = self.encoder.transform(col).toarray()\n",
    "        for i in range(self.X_train.shape[0]):\n",
    "            self.X_train[i][index] = list(one_hot_data[i])\n",
    "            \n",
    "    def OneHotEncodeTest(self, index):\n",
    "        col = np.array(self.X_test[:,index]).reshape(-1, 1)\n",
    "        self.encoder.fit(col)\n",
    "        one_hot_data = self.encoder.transform(col).toarray()\n",
    "        for i in range(self.X_test.shape[0]):\n",
    "            self.X_test[i][index] = list(one_hot_data[i])\n",
    "    \n",
    "    def getCorpus(self, index):\n",
    "        col = np.array(self.X_train[:,index]).reshape(-1, 1)\n",
    "        col = [str(x) for x in col]\n",
    "\n",
    "        row_strings = [''.join(row) for row in col]\n",
    "        final_string = ''.join(row_strings)\n",
    "\n",
    "        tokens = word_tokenize(final_string)\n",
    "\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        final_string = np.unique(stemmed_tokens)\n",
    "\n",
    "        self.corpus.append(final_string.tolist())\n",
    "\n",
    "    def combineCorpus(self):\n",
    "        i = 1\n",
    "        while i < len(self.corpus):\n",
    "            for x in self.corpus[i]:\n",
    "                self.corpus[0].append(str(x))\n",
    "            i+=1\n",
    "        self.corpus = self.corpus[0]\n",
    "        self.corpus = np.unique(self.corpus).tolist()\n",
    "        self.corpus = pd.DataFrame(self.corpus, columns=['words'])\n",
    "\n",
    "    def vectorizeTrain(self, index):\n",
    "        if self.tfidf is None:\n",
    "            self.tfidf = self.vectorizer.fit_transform(self.corpus['words'].values.astype('U'))\n",
    "        for i in range(self.X_train.shape[0]):\n",
    "            if type(self.X_train[i][index]) != type('str'):\n",
    "                self.X_train[i][index] = \"\"\n",
    "            self.X_train[i][index] = (self.vectorizer.transform([self.X_train[i][index]]))\n",
    "    \n",
    "    def vectorizeTest(self, index):\n",
    "        if self.tfidf is None:\n",
    "            self.tfidf = self.vectorizer.fit_transform(self.corpus['words'].values.astype('U'))\n",
    "        for i in range(self.X_test.shape[0]):\n",
    "            if type(self.X_test[i][index]) != type('str'):\n",
    "                self.X_test[i][index] = \"\"\n",
    "            self.X_test[i][index] = (self.vectorizer.transform([self.X_test[i][index]]))\n",
    "\n",
    "        return self.tfidf.toarray()\n",
    "    \n",
    "\n",
    "    def makeCorpus(self, col_num):\n",
    "        \n",
    "        col_train = self.X_train[:, col_num]\n",
    "        col_test = self.X_test[:, col_num]\n",
    "\n",
    "        \n",
    "        stemmer = PorterStemmer()\n",
    "        for col in [col_train, col_test]:\n",
    "            for sentence in col:\n",
    "                words = word_tokenize(sentence.lower())\n",
    "                stemmed_words = [stemmer.stem(word) for word in words]\n",
    "                self.corpus.extend(stemmed_words)\n",
    "\n",
    "        self.corpus = list(set(self.corpus))\n",
    "\n",
    "    def get_top_words(self, numwords = 100):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit_transform([' '.join(self.corpus)])\n",
    "\n",
    "        feature_names = list(vectorizer.vocabulary_.keys())\n",
    "        tfidf_scores = vectorizer.transform([' '.join(self.corpus)])\n",
    "\n",
    "\n",
    "        top_keywords_idx = tfidf_scores.toarray()[0].argsort()[::-1][:numwords]\n",
    "        top_keywords = [feature_names[i] for i in top_keywords_idx]\n",
    "\n",
    "        return top_keywords\n",
    "    \n",
    "    def count_top_words(self):\n",
    "\n",
    "        top_keywords = self.get_top_words()\n",
    "\n",
    "        count_train = np.zeros((self.X_train.shape[0], len(top_keywords)))\n",
    "        count_test = np.zeros((self.X_test.shape[0], len(top_keywords)))\n",
    "\n",
    "        for i, keyword in enumerate(top_keywords):\n",
    "\n",
    "\n",
    "            for j, sentence in enumerate(self.X_train[:, 1]):\n",
    "                words = word_tokenize(sentence.lower())\n",
    "                stemmed_words = [PorterStemmer().stem(word) for word in words]\n",
    "                count_train[j][i] = stemmed_words.count(keyword)\n",
    "\n",
    "\n",
    "            for j, sentence in enumerate(self.X_train[:, 2]):\n",
    "                words = word_tokenize(sentence.lower())\n",
    "                stemmed_words = [PorterStemmer().stem(word) for word in words]\n",
    "                count_train[j][i] += stemmed_words.count(keyword)\n",
    "\n",
    "\n",
    "            for j, sentence in enumerate(self.X_test[:, 1]):\n",
    "                words = word_tokenize(sentence.lower())\n",
    "                stemmed_words = [PorterStemmer().stem(word) for word in words]\n",
    "                count_test[j][i] += stemmed_words.count(keyword)\n",
    "\n",
    "            \n",
    "            for j, sentence in enumerate(self.X_test[:, 2]):\n",
    "                words = word_tokenize(sentence.lower())\n",
    "                stemmed_words = [PorterStemmer().stem(word) for word in words]\n",
    "                count_test[j][i] += stemmed_words.count(keyword)\n",
    "\n",
    "        self.X_train = np.concatenate((self.X_train, count_train), axis=1)\n",
    "        self.X_test = np.concatenate((self.X_test, count_test), axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ded7c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "newsData = NewsData()\n",
    "newsData.OneHotEncodeTrain(3)\n",
    "newsData.OneHotEncodeTrain(4)\n",
    "\n",
    "print(\"1\")\n",
    "newsData.OneHotEncodeTest(3)\n",
    "newsData.OneHotEncodeTest(4)\n",
    "print(\"2\")\n",
    "newsData.makeCorpus(1)\n",
    "newsData.makeCorpus(2)\n",
    "newsData.get_top_words(25)\n",
    "newsData.count_top_words()\n",
    "newsData.X_train = np.delete(newsData.X_train, 1, axis=1)\n",
    "newsData.X_train = np.delete(newsData.X_train, 1, axis=1)\n",
    "# np.concatenate((newsData.X_train, newsData.X_train[]), axis=1)\n",
    "newsData.X_train = np.delete(newsData.X_train, 1, axis=1)\n",
    "newsData.X_train = np.delete(newsData.X_train, 1, axis=1)\n",
    "newsData.X_test = np.delete(newsData.X_test, 1, axis=1)\n",
    "newsData.X_test = np.delete(newsData.X_test, 1, axis=1)\n",
    "newsData.X_test = np.delete(newsData.X_test, 1, axis=1)\n",
    "newsData.X_test = np.delete(newsData.X_test, 1, axis=1)\n",
    "# newsData.combineCorpus()\n",
    "# print(\"3\")\n",
    "# newsData.vectorizeTrain(1)\n",
    "# newsData.vectorizeTrain(2)\n",
    "# print(\"4\")\n",
    "# newsData.vectorizeTest(1)\n",
    "# newsData.vectorizeTest(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb263e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "for i in newsData.X_train[0]:\n",
    "\n",
    "    print(type(i))\n",
    "\n",
    "# print(newsData.X_test[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09420637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866.0\n",
      "0.6418407508325764\n",
      "-0.00368284781868\n",
      "0.123031373031435\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for i in newsData.X_train[1]:\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f948bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to mess with the data more first to start using this code \n",
    "DTC = DecisionTreeClassifier(max_depth= 3)\n",
    "DTC.fit(newsData.X_train[:, 1:], newsData.y_train)\n",
    "\n",
    "dtc_pred = DTC.predict(newsData.X_test[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=100, max_depth = 3)\n",
    "RF.fit(newsData.X_train[:, 1:], newsData.y_train)\n",
    "\n",
    "rf_pred = RF.predict(newsData.X_test[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44293167",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADA = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=7), n_estimators=50)\n",
    "ADA.fit(newsData.X_train[:, 1:], newsData.y_train)\n",
    "\n",
    "ada_pred = ADA.predict(newsData.X_test[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f27956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ac711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7368421052631579, 0.6842105263157895, 0.5526315789473685]\n",
      "[0.7368421052631579, 0.6842105263157895, 0.5526315789473685]\n",
      "[-0.5136268343815515, -0.4339622641509435, -0.8322851153039836]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "preds.append(dtc_pred)\n",
    "preds.append(rf_pred)\n",
    "preds.append(ada_pred)\n",
    "\n",
    "\n",
    "acc = []\n",
    "f1 = []\n",
    "r2 = []\n",
    "\n",
    "for p in preds:\n",
    "    acc.append(accuracy_score(newsData.y_test, p))\n",
    "    f1.append(f1_score(newsData.y_test, p, average='micro'))\n",
    "    r2.append(r2_score(newsData.y_test, p))\n",
    "\n",
    "print(acc)\n",
    "print(f1)\n",
    "print(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
